<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>tedata.scraper API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tedata.scraper</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tedata.scraper.find_element_header_match"><code class="name flex">
<span>def <span class="ident">find_element_header_match</span></span>(<span>soup: bs4.BeautifulSoup, selector: str, match_text: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_element_header_match(soup: BeautifulSoup, selector: str, match_text: str):
    &#34;&#34;&#34;Find .card-header element with text matching search_text&#34;&#34;&#34;
    elements = soup.select(selector)
    print(&#34;Elements found from selector, number of them: &#34;, len(elements))
    for ele in elements:
        print(&#34;\n&#34;, str(ele), &#34;\n&#34;)
        if str(ele.header.text).strip().lower() == match_text.lower():
            print(&#34;Match found: &#34;, ele.header.text)
            return ele
    return None</code></pre>
</details>
<div class="desc"><p>Find .card-header element with text matching search_text</p></div>
</dd>
<dt id="tedata.scraper.scrape_chart"><code class="name flex">
<span>def <span class="ident">scrape_chart</span></span>(<span>url: str = 'https://tradingeconomics.com/united-states/business-confidence',<br>id: str = None,<br>country: str = 'united-states',<br>scraper: <a title="tedata.scraper.TE_Scraper" href="#tedata.scraper.TE_Scraper">TE_Scraper</a> = None,<br>driver: <module 'selenium.webdriver' from '/Users/jamesbishop/Documents/miniconda3/envs/bm/lib/python3.11/site-packages/selenium/webdriver/__init__.py'> = None,<br>headless: bool = True,<br>browser: str = 'firefox') ‑> <a title="tedata.scraper.TE_Scraper" href="#tedata.scraper.TE_Scraper">TE_Scraper</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_chart(url: str = &#34;https://tradingeconomics.com/united-states/business-confidence&#34;, 
                 id: str = None,
                 country: str = &#34;united-states&#34;,
                 scraper: TE_Scraper = None,
                 driver: webdriver = None, 
                 headless: bool = True, 
                 browser: str = &#39;firefox&#39;) -&gt; TE_Scraper:
    
    &#34;&#34;&#34; This convenience function will scrape a chart from Trading Economics and return a TE_Scraper object with the series data in
    the &#39;series&#39; attribute. Metadata is also retreived and stored in the &#39;series_metadata&#39; &amp; &#39;metadata&#39; attributes.
    
    *There are multiple ways to use this function:*

    - Supply URL of the chart to scrape OR supply country + id of the chart to scrape. country and id are just the latter parts of the 
    full chart URL. e.g for URL: &#39;https://tradingeconomics.com/united-states/business-confidence&#39;, we could instead use country=&#39;united-states&#39; 
    and id=&#39;business-confidence&#39;. You can supply only id and default country is &#39;united-states&#39;.
    - You can leave scraper and driver as None and the function will create a new TE_Scraper object for that URL and use it to scrape the data.
    You can however save time by passing either a scraper object or a driver object to the function. Best to pass a driver object
    for fastest results.
    
    **Parameters**

    - url (str): The URL of the chart to scrape.
    - id (str): The id of the chart to scrape. This is the latter part of the URL after the country name.
    - country (str): The country of the chart to scrape. Default is &#39;united-states&#39;.
    - scraper (TE_Scraper): A TE_Scraper object to use for scraping the data. If this is passed, the function will not create a new one.
    - driver (webdriver): A Selenium WebDriver object to use for scraping the data. If this is passed, the function will not create a new one. If 
    scraper and driver are both passed, the webdriver of the scraper object will be used rather than the supplied webdriver.
    - headless (bool): Whether to run the browser in headless mode (display no window).
    - browser (str): The browser to use, either &#39;chrome&#39; or &#39;firefox&#39;.

    **Returns**
    - TE_Scraper object with the scraped data or None if an error occurs.
    &#34;&#34;&#34;

    ## Parameters that monitor progress....
    loaded_page = False; clicked_button = False; yaxis = None; series = None; x_index = None; scaled_series = None; datamax = None; datamin = None

    if scraper is not None:
        sel = scraper
        if driver is None:
            driver = scraper.driver
        else:
            scraper.driver = driver
    else:
        sel = TE_Scraper(driver = driver, browser = browser, headless = headless, use_existing_driver=True)

    if id is not None:
        url = f&#34;https://tradingeconomics.com/{country}/{id}&#34;

    if sel.load_page(url):
        logger.info(f&#34;Page at {url} loaded successfully.&#34;)
        loaded_page = True
        logger.debug(f&#34;Page loaded successfully {url}&#34;)
    else:
        print(&#34;Error loading page at: &#34;, url)
        logger.debug(f&#34;Error loading page at: {url}&#34;)
        return None

    if sel.click_button(sel.find_max_button()):  ## This is the &#34;MAX&#34; button on the Trading Economics chart to set the chart to max length.
        print(&#34;Clicked the MAX button successfully.&#34;)
        logger.debug(f&#34;Clicked the MAX button successfully.&#34;)
        clicked_button = True
    else:
        print(&#34;Error clicking the MAX button.&#34;)
        logger.debug(f&#34;Error clicking the MAX button.&#34;)
        return None
    
    time.sleep(1)
    try:
        yaxis = sel.get_y_axis()
        print(&#34;Successfully scraped y-axis values from the chart:&#34;, &#34; \n&#34;, yaxis) 
        logger.debug(f&#34;Successfully scraped y-axis values from the chart.&#34;) 
    except Exception as e:
        print(f&#34;Error scraping y-axis: {str(e)}&#34;)
        logger.debug(f&#34;Error scraping y-axis: {str(e)}&#34;)
    
    try:
        sel.get_element()
        series = sel.series_from_element(invert_the_series=True, return_series=True)
        print(&#34;Successfully scraped raw pixel co-ordinate series from the path element in chart:&#34;, &#34; \n&#34;, series)
        time.sleep(1)
    except Exception as e:
        print(f&#34;Error scraping y-axis: {str(e)}&#34;)
        logger.debug(f&#34;Error scraping y-axis: {str(e)}&#34;)

    try:
        x_index = sel.make_x_index(return_index=True)
        time.sleep(1)
    except Exception as e:
        print(f&#34;Error creating date index: {str(e)}&#34;)
        logger.debug(f&#34;Error creating date index: {str(e)}&#34;)

    try:
        #datamax, datamin = sel.get_datamax_min()   
        scaled_series = sel.scale_series()   
        logger.info(&#34;Successfully scaled series.&#34;)    
        logger.debug(&#34;Successfully scaled series.&#34;) 
    except Exception as e:
        print(f&#34;Error scaling series: {str(e)}&#34;)
        logger.debug(f&#34;Error scaling series: {str(e)}&#34;)
    
    if loaded_page and clicked_button and yaxis is not None and series is not None and x_index is not None and scaled_series is not None: #and datamax is not None and datamin is not None:
        print(&#34;Successfully scraped time-series from chart at: &#34;, url, &#34; \n&#34;, sel.series, &#34;now getting some metadata...&#34;)
        sel.scrape_metadata()
        print(&#34;Check the metadata: &#34;, sel.series_metadata, &#34;\nScraping complete! Happy pirating yo!&#34;)
        logger.debug(f&#34;Scraping complete, data series retrieved successfully from chart at: {url}&#34;)
        return sel
    else:
        print(&#34;Error scraping chart at: &#34;, url) 
        return None</code></pre>
</details>
<div class="desc"><p>This convenience function will scrape a chart from Trading Economics and return a TE_Scraper object with the series data in
the 'series' attribute. Metadata is also retreived and stored in the 'series_metadata' &amp; 'metadata' attributes.</p>
<p><em>There are multiple ways to use this function:</em></p>
<ul>
<li>Supply URL of the chart to scrape OR supply country + id of the chart to scrape. country and id are just the latter parts of the
full chart URL. e.g for URL: 'https://tradingeconomics.com/united-states/business-confidence', we could instead use country='united-states'
and id='business-confidence'. You can supply only id and default country is 'united-states'.</li>
<li>You can leave scraper and driver as None and the function will create a new TE_Scraper object for that URL and use it to scrape the data.
You can however save time by passing either a scraper object or a driver object to the function. Best to pass a driver object
for fastest results.</li>
</ul>
<p><strong>Parameters</strong></p>
<ul>
<li>url (str): The URL of the chart to scrape.</li>
<li>id (str): The id of the chart to scrape. This is the latter part of the URL after the country name.</li>
<li>country (str): The country of the chart to scrape. Default is 'united-states'.</li>
<li>scraper (TE_Scraper): A TE_Scraper object to use for scraping the data. If this is passed, the function will not create a new one.</li>
<li>driver (webdriver): A Selenium WebDriver object to use for scraping the data. If this is passed, the function will not create a new one. If
scraper and driver are both passed, the webdriver of the scraper object will be used rather than the supplied webdriver.</li>
<li>headless (bool): Whether to run the browser in headless mode (display no window).</li>
<li>browser (str): The browser to use, either 'chrome' or 'firefox'.</li>
</ul>
<p><strong>Returns</strong>
- TE_Scraper object with the scraped data or None if an error occurs.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tedata.scraper.TE_Scraper"><code class="flex name class">
<span>class <span class="ident">TE_Scraper</span></span>
<span>(</span><span>driver: <module 'selenium.webdriver' from '/Users/jamesbishop/Documents/miniconda3/envs/bm/lib/python3.11/site-packages/selenium/webdriver/__init__.py'> = None,<br>use_existing_driver: bool = False,<br>browser: Literal['chrome', 'firefox'] = 'firefox',<br>headless: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TE_Scraper(object):
    &#34;&#34;&#34;Class for scraping data from Trading Economics website. This is the main workhorse of the module.
    It is designed to scrape data from the Trading Economics website using Selenium and BeautifulSoup.
    It can load a page, click buttons, extract data from elements, and plot the extracted data.

    **Init Parameters:** 

    - driver (webdriver): A Selenium WebDriver object, can put in an active one or make a new one for a new URL.
    - use_existing_driver (bool): Whether to use an existing driver in the namespace. If True, the driver parameter is ignored.
    - browser (str): The browser to use for scraping, either &#39;chrome&#39; or &#39;firefox&#39;.
    - headless (bool): Whether to run the browser in headless mode (show no window).
    &#34;&#34;&#34;

    # Define browser type with allowed values
    BrowserType = Literal[&#34;chrome&#34;, &#34;firefox&#34;]
    def __init__(self, 
                 driver: webdriver = None, 
                 use_existing_driver: bool = False,
                 browser: BrowserType = &#34;firefox&#34;, 
                 headless: bool = True):
        
        self.browser = browser
        self.headless = headless

        active = utils.find_active_drivers() 
        if len(active) &lt;= 1:
            use_existing_driver = False

        if driver is None and not use_existing_driver:
            if browser == &#34;chrome&#34;:
                print(&#34;Chrome browser not supported yet. Please use Firefox.&#34;)
                logger.debug(f&#34;Chrome browser not supported yet. Please use Firefox.&#34;)
                return None
                # self.driver = utils.setup_chrome_driver(headless = headless)
            elif browser == &#34;firefox&#34;:
                options = webdriver.FirefoxOptions()
                if headless:
                    options.add_argument(&#39;--headless&#39;)
                self.driver = utils.TimestampedFirefox(options=options)
            else:
                logger.debug(f&#34;Error: Unsupported browser! Use &#39;chrome&#39; or &#39;firefox&#39;.&#34;)
                raise ValueError(&#34;Unsupported browser! Use &#39;chrome&#39; or &#39;firefox&#39;.&#34;)
            logger.debug(f&#34;New {browser} driver created.&#34;)
        elif use_existing_driver:   ## May want to change this later to make sure a scraper doesn&#39;t steal the driver from a search object.
            self.driver = active[-1][0]
            logger.debug(f&#34;Using existing {browser} driver.&#34;)
        else:
            self.driver = driver
            logger.debug(f&#34;Using supplied driver.&#34;)
        
        self.wait = WebDriverWait(self.driver, timeout=10)
        self.start_end = None
    
    def load_page(self, url, wait_time=5):
        &#34;&#34;&#34;Load page and wait for it to be ready&#34;&#34;&#34;

        self.last_url = url
        self.series_name = url.split(&#34;/&#34;)[-1].replace(&#34;-&#34;, &#34; &#34;)
        try:
            self.driver.get(url)
            logger.debug(f&#34;Page loaded successfully: {url}&#34;)
            logger.info(f&#34;WebPage at {url} loaded successfully.&#34;)
            time.sleep(wait_time)  # Basic wait for page load
            self.full_page = self.get_page_source()
            self.page_soup = BeautifulSoup(self.full_page, &#39;html.parser&#39;)
            return True
        except Exception as e:
            print(f&#34;Error loading page: {str(e)}&#34;)
            logger.debug(f&#34;Error loading page: {str(e)}&#34;)
            return False
    
    def click_button(self, selector, selector_type=By.CSS_SELECTOR):
        &#34;&#34;&#34;Click button and wait for response...&#34;&#34;&#34;

        try:
            # Wait for element to be clickable
            button = self.wait.until(
                EC.element_to_be_clickable((selector_type, selector))
            )
            # Scroll element into view
            #self.driver.execute_script(&#34;arguments[0].scrollIntoView(true);&#34;, button)
            time.sleep(1)  # Brief pause after scroll
            button.click()
            logger.info(&#34;Button clicked successfully, waiting 2s for response...&#34;)
            logger.debug(f&#34;Button clicked successfully: {selector}&#34;)
            time.sleep(2)
            return True
        except TimeoutException:
            logger.info(f&#34;Button not found or not clickable: {selector}&#34;)
            logger.debug(f&#34;Button not found or not clickable: {selector}&#34;)
            return False
        except Exception as e:
            logger.info(f&#34;Error clicking button: {str(e)}&#34;)
            logger.debug(f&#34;Error clicking button: {str(e)}&#34;)
            return False

    def find_max_button(self, selector: str = &#34;#dateSpansDiv&#34;):
        &#34;&#34;&#34;Find the button on the chart that selects the maximum date range and return the CSS selector for it.&#34;&#34;&#34;

        try:
            buts = self.page_soup.select_one(selector)
            i = 1
            for res in buts.find_all(&#34;a&#34;):
                #print(res.text)
                if res.text.upper() == &#34;MAX&#34;:
                    max_selector = res.get(&#34;class&#34;)
                    if isinstance(max_selector, list):
                        max_selector = max_selector[0]
                    fin_selector = &#34;a.&#34; + max_selector + f&#34;:nth-child({i})&#34;
                    logger.debug(f&#34;MAX button found for chart at URL: {self.last_url}, selector: {fin_selector}&#34;)
                i += 1
            
            return fin_selector
        except Exception as e:
            print(f&#34;Error finding date spans buttons: {str(e)}&#34;)
            logger.debug(f&#34;Error finding date spans buttons: {str(e)}&#34;)
            return None

    def get_element(self, selector: str = &#34;.highcharts-series path&#34;, selector_type=By.CSS_SELECTOR):
        &#34;&#34;&#34;Find element by selector. The data trace displayed on a Trading Economics chart is a PATH element in the SVG chart.
        This is selected using the CSS selector &#34;.highcharts-series path&#34; by default. The element is stored in the &#39;current_element&#39; attribute.
        It can be used to select other elements on the chart as well and assign that to current element attribute.
        
        **Parameters:**
        - selector (str): The CSS selector for the element to find.
        - selector_type (By): The type of selector to use, By.CSS_SELECTOR by default.

        **Returns:**
        - element: The found element or None if not found.
        &#34;&#34;&#34;
        try:
            element = self.wait.until(
                EC.presence_of_element_located((selector_type, selector))
            )
            self.current_element = element
            logger.debug(f&#34;Element found and assigned to current_element attribute: {selector}&#34;)
            return element
        except TimeoutException:
            print(f&#34;Element not found: {selector}&#34;)
            logger.debug(f&#34;Element not found: {selector}&#34;)
            return None
        except Exception as e:
            print(f&#34;Error finding element: {str(e)}&#34;)
            logger.debug(f&#34;Error finding element: {str(e)}&#34;)
            return None
        
    def series_from_element(self, element: str = None, invert_the_series: bool = True, return_series: bool = False):
        &#34;&#34;&#34;Extract series data from element text. This extracts the plotted series from the svg chart by taking the PATH 
        element of the data tarace on the chart. Series values are pixel co-ordinates on the chart.

        **Parameters:**

        - element (str): The element to extract data from. Will use self.current_element if not provided.
        - invert_the_series (bool): Whether to invert the series values.

        **Returns:**

        - series (pd.Series): The extracted series data.
        &#34;&#34;&#34;

        if element is None:
            element = self.current_element
        
        datastrlist = element.get_attribute(&#34;d&#34;).split(&#34; &#34;)
        ser = pd.Series(datastrlist)
        ser_num = pd.to_numeric(ser, errors=&#39;coerce&#39;).dropna()

        exvals = ser_num[::2]; yvals = ser_num[1::2]
        exvals = exvals.sort_values().to_list()
        yvals = yvals.to_list()
        series = pd.Series(yvals, index = exvals, name = &#34;Extracted Series&#34;)

        if invert_the_series:
            series = utils.invert_series(series, max_val = self.y_axis.index.max())
        self.series = series

        self.pix0 = self.series.iloc[0]; self.pix1 = self.series.iloc[-1]
        logger.debug(f&#34;Raw data series extracted successfully: {series.head()}&#34;)
        logger.info(f&#34;Raw data series extracted successfully.&#34;)
        if return_series:
            return series
    
    def get_datamax_min(self):
        &#34;&#34;&#34;Get the max and min data values for the series using y-axis values... This is deprecated and not used in the current version of the code.&#34;&#34;&#34;
        
        logger.debug(f&#34;get_datamax_min method, axisY0 = {self.y_axis.iloc[0]}, axisY1 = {self.y_axis.iloc[-1]}&#34;)
        px_range = self.y_axis.index[-1] - self.y_axis.index[0]
        labrange = self.y_axis.iloc[-1] - self.y_axis.iloc[0]
        self.unit_per_pix_alt2 = labrange/px_range
        print(&#34;unit_per_pix: &#34;, self.unit_per_pix)
        logger.debug(f&#34;unit_per_pix: {self.unit_per_pix}, alt2: {self.unit_per_pix_alt2}&#34;)
        self.datamax = round(self.y_axis.iloc[-1] - (self.y_axis.index[-1] - self.series.max())*self.unit_per_pix, 3)
        self.datamin = round(self.y_axis.iloc[0] + (self.series.min()-self.y_axis.index[0])*self.unit_per_pix, 3)
        print(&#34;datamax: &#34;, self.datamax, &#34;datamin: &#34;, self.datamin)
        logger.debug(f&#34;datamax: {self.datamax}, datamin: {self.datamin}&#34;)
        return self.datamax, self.datamin
    
    def scale_series(self, right_way_up: bool = True):
        &#34;&#34;&#34;Scale the series using the first and last values from the series pulled from the tooltip box. Uses the y axis limits and the max and min of the y axis
        to determine the scaling factor to convert pixel co-ordinates to data values. The scaling factor is stored in the self.axlims_upp attribute.&#34;&#34;&#34;

        if not right_way_up:
            max_val = self.y_axis.index.max()  # This should be the top pixel of the chart.
            self.series = utils.invert_series(self.series, max_val = max_val)

        if hasattr(self, &#34;start_end&#34;):
            y0 = self.start_end[&#34;start_value&#34;]; y1 = self.start_end[&#34;end_value&#34;]
            pix0 = self.series.iloc[0]; pix1 = self.series.iloc[-1]
            logger.debug(f&#34;scale_series method: Start value, end value: {y0}, {y1}, {pix0}, {pix1}, {pix0}, {pix1}&#34;)
            
            self.unit_per_px_alt = abs(y1 - y0) / abs(pix1 - pix0)  # Calculated from the start and end datapoints.
            
            if not hasattr(self, &#34;axis_limits&#34;):
                self.axis_limits = self.extract_axis_limits()
            ## Turns out that this formulation below is the best way to calculate the scaling factor for the chart.
            self.axlims_upp = (self.y_axis.iloc[-1] - self.y_axis.iloc[0]) / (self.axis_limits[&#34;y_max&#34;] - self.axis_limits[&#34;y_min&#34;])

            # if the start and end points are at similar values this will be problematic though. 
            logger.debug(f&#34;Start value, end value: {y0}, {y1}, pix0, pix1: {pix0}, {pix1}, &#34;
                         f&#34;data units per chart pixel from start &amp; end points: {self.unit_per_px_alt}, &#34;
                         f&#34;unit_per_pix calculated from the y axis ticks: {self.unit_per_pix}, &#34;
                         f&#34;inverse of that: {1/self.unit_per_pix}, &#34;
                         f&#34;unit_per_pix from axis limits and self.y_axis (probably best way): {self.axlims_upp}&#34;)

            self.unscaled_series = self.series.copy()
            ##Does the Y axis cross zero? Where is the zero point??
            x_intercept = utils.find_zero_crossing(self.series)

            if x_intercept:
                logger.debug(f&#34;Y axis Series does cross zero at:  {x_intercept}&#34;)
                pix0 = x_intercept

            for i in range(len(self.series)):
                self.series.iloc[i] = (self.series.iloc[i] - pix0)*self.axlims_upp + y0
    
            self.series = self.series
        else:
            print(&#34;start_end not found, run get_datamax_min() first.&#34;)
            logger.debug(&#34;start_end not found, run get_datamax_min() first.&#34;)
            return

        return self.series
    
    def get_xlims_from_tooltips(self, force_rerun: bool = False):
        &#34;&#34;&#34; Use the get_tooltip class to get the start and end dates and some other points of the time series using the tooltip box displayed on the chart.
        Takes the latest num_points points from the chart and uses them to determine the frequency of the time series. The latest data is used
        in case the earlier data is of lower frequency which can sometimes occurr.
        
        **Parameters:**
        
        - force_rerun (bool): Whether to force a rerun of the method to get the start and end dates and frequency of the time series again. The method
        will not run again by default if done a second time and start_end and frequency attributes are already set. If the first run resulted in erroneous
        assignation of these attributes, set this to True to rerun the method. However, something may need to be changed if it is not working...&#34;&#34;&#34;

        if hasattr(self, &#34;tooltip_scraper&#34;):
            pass    
        else: 
            self.tooltip_scraper = utils.get_tooltip(driver=self.driver, chart_x=335.5, chart_y=677.0)  #Note: update this later to use self.width and height etc...
        
        if hasattr(self, &#34;start_end&#34;) and self.start_end is not None and hasattr(self, &#34;frequency&#34;) and self.frequency is not None and not force_rerun:
            return
        else:
            time.sleep(1)
            data_points, num_points = self.tooltip_scraper.scrape_dates_from_tooltips(num_points=7)
            logger.info(f&#34;Scraped {len(data_points)} data points from the chart, num_points target was {num_points}&#34;)
            
            if len(data_points) &gt; num_points:
                logger.debug(&#34;Successfully scraped start and end dates plus other data points to determine frequency of the time-series...&#34;)
                self.start_end = {
                &#39;start_date&#39;: data_points[-1][&#34;date&#34;],
                &#39;end_date&#39;: data_points[0][&#34;date&#34;],
                &#39;start_value&#39;: data_points[-1][&#34;value&#34;], 
                &#39;end_value&#39;: data_points[0][&#34;value&#34;]
                }
                dates = [point[&#34;date&#34;] for point in data_points][-2::-1]
                values = [point[&#34;value&#34;] for point in data_points][-2::-1]
                self.ripped_points = {&#34;dates&#34;: dates, &#34;values&#34;: values}

                diff = pd.Series(dates).diff().dropna().mode()[0]
                self.frequency = utils.map_frequency(diff)
                print(f&#34;Time delta between data points in the series appears to be approxiately: {diff.days} days, will use {self.frequency} frequency.&#34;)
                logger.debug(f&#34;Time series frequency appears to be: {diff.days}, {self.frequency}&#34;)

            else:
                logger.info(&#34;Error scraping data from tooltips..&#34;)
                logger.debug(&#34;Error scraping data from tooltips..&#34;)
                return None

    def make_x_index(self, force_rerun: bool = False, return_index: bool = False):
        &#34;&#34;&#34;Make the DateTime Index for the series using the start and end dates scraped from the tooltips. 
        This does a few things and uses Selenium to scrape the dates from the tooltips on the chart as well as
        some more points to determine the frequency of the time series. It will take some time....
        &#34;&#34;&#34;
        
        print(&#34;Using selenium and toltip scraping to construct the date time index for the time-series, this&#39;ll take a bit...&#34;)
        self.get_xlims_from_tooltips(force_rerun = force_rerun)

        if self.start_end is not None:
            logger.info(f&#34;Start and end values scraped from tooltips: {self.start_end}&#34;)
            logger.debug(f&#34;Start and end values scraped from tooltips: {self.start_end}&#34;)
        else:
            print(&#34;Error: Start and end values not found...pulling out....&#34;)
            logger.debug(f&#34;Error: Start and end values not found...pulling out....&#34;)
            return None

        try:
            start_date = self.start_end[&#34;start_date&#34;]; end_date = self.start_end[&#34;end_date&#34;]
            dtIndex = self.dtIndex(start_date=start_date, end_date=end_date, ser_name=self.series_name)
            #print(&#34;Date index created successfully. Take a look at the final series: \n&#34;, dtIndex)
            logger.debug(f&#34;Date index created successfully: {dtIndex.index}&#34;)
            logger.info(f&#34;Date index created successfully.&#34;)
            if return_index:
                return dtIndex.index
        
        except Exception as e:
            print(f&#34;Error creating date index: {str(e)}&#34;)
       
    def get_y_axis(self):
        &#34;&#34;&#34;Get y-axis values from chart to make a y-axis series with tick labels and positions (pixel positions).
        Also gets the limits of both axis in pixel co-ordinates. A series containing the y-axis values and their pixel positions (as index) is assigned
        to the &#34;y_axis&#34; attribute. The &#34;axis_limits&#34; attribute is made too &amp; is  dictionary containing the pixel co-ordinates of the max and min for both x and y axis.
        &#34;&#34;&#34;

        ##Get positions of y-axis gridlines
        y_heights = []
        self.full_chart = self.get_element(selector = &#34;#chart&#34;).get_attribute(&#34;outerHTML&#34;)
        self.chart_soup = BeautifulSoup(self.full_chart, &#39;html.parser&#39;)  #Make a bs4 object from the #chart element of the page.

        ## First get the pixel values of the max and min for both x and y axis.
        self.axis_limits = self.extract_axis_limits()

        ygrid = self.chart_soup.select(&#39;g.highcharts-grid.highcharts-yaxis-grid&#39;)
        gridlines = ygrid[1].findAll(&#39;path&#39;)
        for line in gridlines:
            y_heights.append(float(line.get(&#39;d&#39;).split(&#39; &#39;)[-1]))
        y_heights = sorted(y_heights)

        ##Get y-axis labels
        yax = self.chart_soup.select(&#39;g.highcharts-axis-labels.highcharts-yaxis-labels&#39;)
        textels = yax[1].find_all(&#39;text&#39;)

        # Replace metrc prefixes:
        yaxlabs = [utils.convert_metric_prefix(text.get_text()) if text.get_text().replace(&#39;,&#39;,&#39;&#39;).replace(&#39;.&#39;,&#39;&#39;).replace(&#39;-&#39;,&#39;&#39;).replace(&#39; &#39;,&#39;&#39;).isalnum() else text.get_text() for text in textels]
        logger.debug(f&#34;y-axis labels: {yaxlabs}&#34;)

        # convert to float...
        if any(isinstance(i, str) for i in yaxlabs):
            yaxlabs = [float(&#39;&#39;.join(filter(str.isdigit, i.replace(&#34;,&#34;, &#34;&#34;)))) if isinstance(i, str) else i for i in yaxlabs]
        pixheights = [float(height) for height in y_heights]
        pixheights.sort()

        ##Get px per unit for y-axis
        pxPerUnit = [abs((yaxlabs[i+1]- yaxlabs[i])/(pixheights[i+1]- pixheights[i])) for i in range(len(pixheights)-1)]
        average = sum(pxPerUnit)/len(pxPerUnit)
        self.unit_per_pix = average
        logger.debug(f&#34;Average px per unit for y-axis: {average}&#34;)  #Calculate the scaling for the chart so we can convert pixel co-ordinates to data values.

        yaxis = pd.Series(yaxlabs, index = pixheights, name = &#34;ytick_label&#34;)
        yaxis.index.rename(&#34;pixheight&#34;, inplace = True)
        try:
            yaxis = yaxis.astype(int)
        except:
            pass

        self.y_axis = yaxis
        if self.y_axis is not None:
            logger.debug(f&#34;Y-axis values scraped successfully.&#34;)
            logger.info(f&#34;Y-axis values scraped successfully.&#34;)
        return yaxis
    
    def dtIndex(self, start_date: str, end_date: str, ser_name: str = &#34;Time-series&#34;):
        &#34;&#34;&#34;

        Create a date index for your series in self.series. Will first make an index to cover the full length of your series 
        and then resample to month start freq to match the format on Trading Economics.
        
        **Parameters:**
        - start_date (str) YYYY-MM-DD: The start date of your series
        - end_date (str) YYYY-MM-DD: The end date of your series
        - ser_name (str): The name TO GIVE the series
        &#34;&#34;&#34;

        dtIndex = pd.date_range(start = start_date, end=end_date, periods=len(self.series))
        new_ser = pd.Series(self.series.to_list(), index = dtIndex, name = ser_name)
        if hasattr(self, &#34;frequency&#34;):
            new_ser = new_ser.resample(self.frequency).first()
        else:
            new_ser = new_ser.resample(&#34;MS&#34;).first()  ## Use First to match the MS freq.
        self.series = new_ser
        return new_ser

    def extract_axis_limits(self):
        &#34;&#34;&#34;Extract axis limits from the chart in terms of pixel co-ordinates.&#34;&#34;&#34;
        logger.debug(f&#34;Extracting axis limits from the chart...&#34;)
        try:
            # Extract axis elements
            yax = self.chart_soup.select_one(&#34;g.highcharts-axis.highcharts-yaxis path.highcharts-axis-line&#34;)
            xax = self.chart_soup.select_one(&#34;g.highcharts-axis.highcharts-xaxis path.highcharts-axis-line&#34;)
            
            ylims = yax[&#34;d&#34;].replace(&#34;M&#34;, &#34;&#34;).replace(&#34;L&#34;, &#34;&#34;).strip().split(&#34; &#34;)
            ylims = [float(num) for num in ylims if len(num) &gt; 0][1::2]
            logger.debug(f&#34;yax: {ylims}&#34;)

            xlims = xax[&#34;d&#34;].replace(&#34;M&#34;, &#34;&#34;).replace(&#34;L&#34;, &#34;&#34;).strip().split(&#34; &#34;)
            xlims = [float(num) for num in xlims if len(num) &gt; 0][0::2]
            logger.debug(f&#34;xax: {xlims}&#34;)
            
            axis_limits = {
                &#39;x_min&#39;: xlims[0],
                &#39;x_max&#39;: xlims[1],
                &#39;y_min&#39;: ylims[0],
                &#39;y_max&#39;: ylims[1]
            }
            
            return axis_limits
        except Exception as e:
            print(f&#34;Error extracting axis limits: {str(e)}&#34;)
            logger.debug(f&#34;Error extracting axis limits: {str(e)}&#34;)
            return None
    
    def plot_series(self, annotation_text: str = None, dpi: int = 300, ann_box_pos: tuple = (0, - 0.23)):
        &#34;&#34;&#34;
        Plots the time series data using pandas with plotly as the backend. Plotly is set as the pandas backend in __init__.py for tedata.
        If you want to use matplotlib or other plotting library don&#39;t use this method, plot the series attribute data directly. If using jupyter
        you can set 

        **Parameters**
        - annotation_text (str): Text to display in the annotation box at the bottom of the chart. Default is None. If None, the default annotation text
        will be created from the metadata.
        - dpi (int): The resolution of the plot in dots per inch. Default is 300.
        - ann_box_pos (tuple): The position of the annotation box on the chart. Default is (0, -0.23) which is bottom left.

        **Returns** None
        &#34;&#34;&#34;

        fig = self.series.plot()  # Plot the series using pandas, plotly needs to be set as the pandas plotting backend.

         # Existing title and label logic
        if hasattr(self, &#34;series_metadata&#34;):
            title = str(self.series_metadata[&#34;country&#34;]).capitalize() + &#34;: &#34; + str(self.series_metadata[&#34;title&#34;]).capitalize()
            ylabel = str(self.series_metadata[&#34;units&#34;]).capitalize()
            
            # Create default annotation text from metadata
            if annotation_text is None:
                annotation_text = (
                    f&#34;Source: {self.series_metadata[&#39;source&#39;]}&lt;br&gt;&#34;
                    f&#34;Original Source: {self.series_metadata[&#39;original_source&#39;]}&lt;br&gt;&#34;
                    f&#34;Frequency: {self.series_metadata[&#39;frequency&#39;]}&#34;
                )
        else:
            title = &#34;Time Series Plot&#34;
            ylabel = &#34;Value&#34;
            annotation_text = annotation_text or &#34;Source: Trading Economics&#34;

        # Add text annotation to bottom left
        fig.add_annotation(
            text=annotation_text,
            xref=&#34;paper&#34;, yref=&#34;paper&#34;,
            x=ann_box_pos[0], y=ann_box_pos[1],
            showarrow=False, font=dict(size=10),
            align=&#34;left&#34;,  xanchor=&#34;left&#34;,
            yanchor=&#34;bottom&#34;, bgcolor=&#34;rgba(255, 255, 255, 0.8)&#34;,
            bordercolor=&#34;black&#34;, borderwidth=1)

        # Label x and y axis
        fig.update_layout(
            legend=dict(
            title_text=&#34;&#34;,  # Remove legend title
            orientation=&#34;h&#34;,
            yanchor=&#34;bottom&#34;,
            y=-0.2,  # Adjust this value to move the legend further down
            xanchor=&#34;center&#34;,
            x=0.5
            ),
            yaxis_title=ylabel,
            xaxis_title=&#34;&#34;,
            title = title)

        # Show the figure
        fig.show()
        self.plot = fig

    def save_plot(self, filename: str = &#34;plot&#34;, save_path: str = os.getcwd(), dpi: int = 300, format: str = &#34;png&#34;):
        &#34;&#34;&#34;Save the plot to a file. The plot must be created using the plot_series method. This method will save the plot as a PNG image file.

        **Parameters**
        - filename (str): The name of the file to save the plot to. Default is &#39;plot.png&#39;.
        - save_path (str): The directory to save the plot to. Default is the current working directory.
        - dpi (int): The resolution of the plot in dots per inch. Default is 300.
        - format (str): The format to save the plot in. Default is &#39;png&#39;. Other options are: &#34;html&#34;, &#34;bmp&#34;, &#34;jpeg&#34;, &#34;jpg&#34;.
        Use &#34;html&#34; to save as an interactive plotly plot.

        :Returns: None
        &#34;&#34;&#34;

        if hasattr(self, &#34;plot&#34;):
            if format == &#34;html&#34;:
                self.plot.write_html(f&#34;{save_path}{fdel}{filename}.html&#34;)
                logger.info(f&#34;Plot saved as {save_path}{fdel}{filename}.html&#34;)
            else:
                self.plot.write_image(f&#34;{save_path}{fdel}{filename}.{format}&#34;, format=format, scale=dpi/100, width = 1400, height = 500)
                logger.info(f&#34;Plot saved as {filename}&#34;)
        else:
            print(&#34;Error: Plot not found. Run plot_series() method to create a plot.&#34;)
            logger.debug(&#34;Error: Plot not found. Run plot_series() method to create a plot.&#34;)

    def scrape_metadata(self):
        &#34;&#34;&#34;Scrape metadata from the page. This method scrapes metadata from the page and stores it in the &#39;metadata&#39; attribute. The metadata
        includes the title, indicator, country, length, frequency, source, , original source, id, start date, end date, min value, and max value of the series.
        It also scrapes a description of the series if available and stores it in the &#39;description&#39; attribute.
        &#34;&#34;&#34;

        self.metadata = {}
        logger.debug(f&#34;Scraping metadata for the series from the page...&#34;)

        try:
            self.metadata[&#34;units&#34;] = self.chart_soup.select_one(&#39;#singleIndChartUnit2&#39;).text
        except Exception as e:
            print(&#34;Units label not found: &#34;, {str(e)})
            self.metadata[&#34;units&#34;] = &#34;a.u&#34;
        
        try:
            self.metadata[&#34;original_source&#34;] = self.chart_soup.select_one(&#39;#singleIndChartUnit&#39;).text
        except Exception as e:
            print(&#34;original_source label not found: &#34;, {str(e)})
            self.metadata[&#34;original_source&#34;] = &#34;unknown&#34;

        if hasattr(self, &#34;series&#34;):
            if hasattr(self, &#34;page_soup&#34;):
                heads = self.page_soup.select(&#34;#ctl00_Head1&#34;)
                self.metadata[&#34;title&#34;] = heads[0].title.text.strip()
            else:
                self.metadata[&#34;title&#34;] = self.last_url.split(&#34;/&#34;)[-1].replace(&#34;-&#34;, &#34; &#34;)  # Use URL if can&#39;t find the title
            self.metadata[&#34;indicator&#34;] = self.last_url.split(&#34;/&#34;)[-1].replace(&#34;-&#34;, &#34; &#34;)  
            self.metadata[&#34;country&#34;] = self.last_url.split(&#34;/&#34;)[-2].replace(&#34;-&#34;, &#34; &#34;) 
            self.metadata[&#34;length&#34;] = len(self.series)
            self.metadata[&#34;frequency&#34;] = self.frequency  
            self.metadata[&#34;source&#34;] = &#34;Trading Economics&#34; 
            self.metadata[&#34;id&#34;] = &#34;/&#34;.join(self.last_url.split(&#34;/&#34;)[-2:])
            self.metadata[&#34;start_date&#34;] = self.series.index[0].strftime(&#34;%Y-%m-%d&#34;)
            self.metadata[&#34;end_date&#34;] = self.series.index[-1].strftime(&#34;%Y-%m-%d&#34;)
            self.metadata[&#34;min_value&#34;] = float(self.series.min())
            self.metadata[&#34;max_value&#34;] = float(self.series.max())
            print(&#34;Series metadata: &#34;, self.metadata)

        try:
            desc_card = self.page_soup.select_one(&#34;#item_definition&#34;)
            header_text = desc_card.select_one(&#39;.card-header&#39;).text.strip()
            if header_text.lower() == self.metadata[&#34;title&#34;].lower():
                self.metadata[&#34;description&#34;] = desc_card.select_one(&#39;.card-body&#39;).text.strip()
            else:
                print(&#34;Description card title does not match series title.&#34;)
                self.metadata[&#34;description&#34;] = &#34;Description not found.&#34;
        except Exception as e:
            print(&#34;Description card not found: &#34;, {str(e)})

        self.series_metadata = pd.Series(self.metadata)
        if self.metadata is not None:
            logger.debug(f&#34;Metadata scraped successfully: {self.metadata}&#34;)

    def get_page_source(self):
        &#34;&#34;&#34;Get current page source after interactions&#34;&#34;&#34;
        return self.driver.page_source
    
    def close(self):
        &#34;&#34;&#34;Clean up resources&#34;&#34;&#34;
        if self.driver:
            self.driver.quit()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()</code></pre>
</details>
<div class="desc"><p>Class for scraping data from Trading Economics website. This is the main workhorse of the module.
It is designed to scrape data from the Trading Economics website using Selenium and BeautifulSoup.
It can load a page, click buttons, extract data from elements, and plot the extracted data.</p>
<p><strong>Init Parameters:</strong> </p>
<ul>
<li>driver (webdriver): A Selenium WebDriver object, can put in an active one or make a new one for a new URL.</li>
<li>use_existing_driver (bool): Whether to use an existing driver in the namespace. If True, the driver parameter is ignored.</li>
<li>browser (str): The browser to use for scraping, either 'chrome' or 'firefox'.</li>
<li>headless (bool): Whether to run the browser in headless mode (show no window).</li>
</ul></div>
<h3>Class variables</h3>
<dl>
<dt id="tedata.scraper.TE_Scraper.BrowserType"><code class="name">var <span class="ident">BrowserType</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="tedata.scraper.TE_Scraper.click_button"><code class="name flex">
<span>def <span class="ident">click_button</span></span>(<span>self, selector, selector_type='css selector')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def click_button(self, selector, selector_type=By.CSS_SELECTOR):
    &#34;&#34;&#34;Click button and wait for response...&#34;&#34;&#34;

    try:
        # Wait for element to be clickable
        button = self.wait.until(
            EC.element_to_be_clickable((selector_type, selector))
        )
        # Scroll element into view
        #self.driver.execute_script(&#34;arguments[0].scrollIntoView(true);&#34;, button)
        time.sleep(1)  # Brief pause after scroll
        button.click()
        logger.info(&#34;Button clicked successfully, waiting 2s for response...&#34;)
        logger.debug(f&#34;Button clicked successfully: {selector}&#34;)
        time.sleep(2)
        return True
    except TimeoutException:
        logger.info(f&#34;Button not found or not clickable: {selector}&#34;)
        logger.debug(f&#34;Button not found or not clickable: {selector}&#34;)
        return False
    except Exception as e:
        logger.info(f&#34;Error clicking button: {str(e)}&#34;)
        logger.debug(f&#34;Error clicking button: {str(e)}&#34;)
        return False</code></pre>
</details>
<div class="desc"><p>Click button and wait for response&hellip;</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    &#34;&#34;&#34;Clean up resources&#34;&#34;&#34;
    if self.driver:
        self.driver.quit()</code></pre>
</details>
<div class="desc"><p>Clean up resources</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.dtIndex"><code class="name flex">
<span>def <span class="ident">dtIndex</span></span>(<span>self, start_date: str, end_date: str, ser_name: str = 'Time-series')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtIndex(self, start_date: str, end_date: str, ser_name: str = &#34;Time-series&#34;):
    &#34;&#34;&#34;

    Create a date index for your series in self.series. Will first make an index to cover the full length of your series 
    and then resample to month start freq to match the format on Trading Economics.
    
    **Parameters:**
    - start_date (str) YYYY-MM-DD: The start date of your series
    - end_date (str) YYYY-MM-DD: The end date of your series
    - ser_name (str): The name TO GIVE the series
    &#34;&#34;&#34;

    dtIndex = pd.date_range(start = start_date, end=end_date, periods=len(self.series))
    new_ser = pd.Series(self.series.to_list(), index = dtIndex, name = ser_name)
    if hasattr(self, &#34;frequency&#34;):
        new_ser = new_ser.resample(self.frequency).first()
    else:
        new_ser = new_ser.resample(&#34;MS&#34;).first()  ## Use First to match the MS freq.
    self.series = new_ser
    return new_ser</code></pre>
</details>
<div class="desc"><p>Create a date index for your series in self.series. Will first make an index to cover the full length of your series
and then resample to month start freq to match the format on Trading Economics.</p>
<p><strong>Parameters:</strong>
- start_date (str) YYYY-MM-DD: The start date of your series
- end_date (str) YYYY-MM-DD: The end date of your series
- ser_name (str): The name TO GIVE the series</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.extract_axis_limits"><code class="name flex">
<span>def <span class="ident">extract_axis_limits</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_axis_limits(self):
    &#34;&#34;&#34;Extract axis limits from the chart in terms of pixel co-ordinates.&#34;&#34;&#34;
    logger.debug(f&#34;Extracting axis limits from the chart...&#34;)
    try:
        # Extract axis elements
        yax = self.chart_soup.select_one(&#34;g.highcharts-axis.highcharts-yaxis path.highcharts-axis-line&#34;)
        xax = self.chart_soup.select_one(&#34;g.highcharts-axis.highcharts-xaxis path.highcharts-axis-line&#34;)
        
        ylims = yax[&#34;d&#34;].replace(&#34;M&#34;, &#34;&#34;).replace(&#34;L&#34;, &#34;&#34;).strip().split(&#34; &#34;)
        ylims = [float(num) for num in ylims if len(num) &gt; 0][1::2]
        logger.debug(f&#34;yax: {ylims}&#34;)

        xlims = xax[&#34;d&#34;].replace(&#34;M&#34;, &#34;&#34;).replace(&#34;L&#34;, &#34;&#34;).strip().split(&#34; &#34;)
        xlims = [float(num) for num in xlims if len(num) &gt; 0][0::2]
        logger.debug(f&#34;xax: {xlims}&#34;)
        
        axis_limits = {
            &#39;x_min&#39;: xlims[0],
            &#39;x_max&#39;: xlims[1],
            &#39;y_min&#39;: ylims[0],
            &#39;y_max&#39;: ylims[1]
        }
        
        return axis_limits
    except Exception as e:
        print(f&#34;Error extracting axis limits: {str(e)}&#34;)
        logger.debug(f&#34;Error extracting axis limits: {str(e)}&#34;)
        return None</code></pre>
</details>
<div class="desc"><p>Extract axis limits from the chart in terms of pixel co-ordinates.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.find_max_button"><code class="name flex">
<span>def <span class="ident">find_max_button</span></span>(<span>self, selector: str = '#dateSpansDiv')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_max_button(self, selector: str = &#34;#dateSpansDiv&#34;):
    &#34;&#34;&#34;Find the button on the chart that selects the maximum date range and return the CSS selector for it.&#34;&#34;&#34;

    try:
        buts = self.page_soup.select_one(selector)
        i = 1
        for res in buts.find_all(&#34;a&#34;):
            #print(res.text)
            if res.text.upper() == &#34;MAX&#34;:
                max_selector = res.get(&#34;class&#34;)
                if isinstance(max_selector, list):
                    max_selector = max_selector[0]
                fin_selector = &#34;a.&#34; + max_selector + f&#34;:nth-child({i})&#34;
                logger.debug(f&#34;MAX button found for chart at URL: {self.last_url}, selector: {fin_selector}&#34;)
            i += 1
        
        return fin_selector
    except Exception as e:
        print(f&#34;Error finding date spans buttons: {str(e)}&#34;)
        logger.debug(f&#34;Error finding date spans buttons: {str(e)}&#34;)
        return None</code></pre>
</details>
<div class="desc"><p>Find the button on the chart that selects the maximum date range and return the CSS selector for it.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.get_datamax_min"><code class="name flex">
<span>def <span class="ident">get_datamax_min</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_datamax_min(self):
    &#34;&#34;&#34;Get the max and min data values for the series using y-axis values... This is deprecated and not used in the current version of the code.&#34;&#34;&#34;
    
    logger.debug(f&#34;get_datamax_min method, axisY0 = {self.y_axis.iloc[0]}, axisY1 = {self.y_axis.iloc[-1]}&#34;)
    px_range = self.y_axis.index[-1] - self.y_axis.index[0]
    labrange = self.y_axis.iloc[-1] - self.y_axis.iloc[0]
    self.unit_per_pix_alt2 = labrange/px_range
    print(&#34;unit_per_pix: &#34;, self.unit_per_pix)
    logger.debug(f&#34;unit_per_pix: {self.unit_per_pix}, alt2: {self.unit_per_pix_alt2}&#34;)
    self.datamax = round(self.y_axis.iloc[-1] - (self.y_axis.index[-1] - self.series.max())*self.unit_per_pix, 3)
    self.datamin = round(self.y_axis.iloc[0] + (self.series.min()-self.y_axis.index[0])*self.unit_per_pix, 3)
    print(&#34;datamax: &#34;, self.datamax, &#34;datamin: &#34;, self.datamin)
    logger.debug(f&#34;datamax: {self.datamax}, datamin: {self.datamin}&#34;)
    return self.datamax, self.datamin</code></pre>
</details>
<div class="desc"><p>Get the max and min data values for the series using y-axis values&hellip; This is deprecated and not used in the current version of the code.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.get_element"><code class="name flex">
<span>def <span class="ident">get_element</span></span>(<span>self, selector: str = '.highcharts-series path', selector_type='css selector')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_element(self, selector: str = &#34;.highcharts-series path&#34;, selector_type=By.CSS_SELECTOR):
    &#34;&#34;&#34;Find element by selector. The data trace displayed on a Trading Economics chart is a PATH element in the SVG chart.
    This is selected using the CSS selector &#34;.highcharts-series path&#34; by default. The element is stored in the &#39;current_element&#39; attribute.
    It can be used to select other elements on the chart as well and assign that to current element attribute.
    
    **Parameters:**
    - selector (str): The CSS selector for the element to find.
    - selector_type (By): The type of selector to use, By.CSS_SELECTOR by default.

    **Returns:**
    - element: The found element or None if not found.
    &#34;&#34;&#34;
    try:
        element = self.wait.until(
            EC.presence_of_element_located((selector_type, selector))
        )
        self.current_element = element
        logger.debug(f&#34;Element found and assigned to current_element attribute: {selector}&#34;)
        return element
    except TimeoutException:
        print(f&#34;Element not found: {selector}&#34;)
        logger.debug(f&#34;Element not found: {selector}&#34;)
        return None
    except Exception as e:
        print(f&#34;Error finding element: {str(e)}&#34;)
        logger.debug(f&#34;Error finding element: {str(e)}&#34;)
        return None</code></pre>
</details>
<div class="desc"><p>Find element by selector. The data trace displayed on a Trading Economics chart is a PATH element in the SVG chart.
This is selected using the CSS selector ".highcharts-series path" by default. The element is stored in the 'current_element' attribute.
It can be used to select other elements on the chart as well and assign that to current element attribute.</p>
<p><strong>Parameters:</strong>
- selector (str): The CSS selector for the element to find.
- selector_type (By): The type of selector to use, By.CSS_SELECTOR by default.</p>
<p><strong>Returns:</strong>
- element: The found element or None if not found.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.get_page_source"><code class="name flex">
<span>def <span class="ident">get_page_source</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_page_source(self):
    &#34;&#34;&#34;Get current page source after interactions&#34;&#34;&#34;
    return self.driver.page_source</code></pre>
</details>
<div class="desc"><p>Get current page source after interactions</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.get_xlims_from_tooltips"><code class="name flex">
<span>def <span class="ident">get_xlims_from_tooltips</span></span>(<span>self, force_rerun: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xlims_from_tooltips(self, force_rerun: bool = False):
    &#34;&#34;&#34; Use the get_tooltip class to get the start and end dates and some other points of the time series using the tooltip box displayed on the chart.
    Takes the latest num_points points from the chart and uses them to determine the frequency of the time series. The latest data is used
    in case the earlier data is of lower frequency which can sometimes occurr.
    
    **Parameters:**
    
    - force_rerun (bool): Whether to force a rerun of the method to get the start and end dates and frequency of the time series again. The method
    will not run again by default if done a second time and start_end and frequency attributes are already set. If the first run resulted in erroneous
    assignation of these attributes, set this to True to rerun the method. However, something may need to be changed if it is not working...&#34;&#34;&#34;

    if hasattr(self, &#34;tooltip_scraper&#34;):
        pass    
    else: 
        self.tooltip_scraper = utils.get_tooltip(driver=self.driver, chart_x=335.5, chart_y=677.0)  #Note: update this later to use self.width and height etc...
    
    if hasattr(self, &#34;start_end&#34;) and self.start_end is not None and hasattr(self, &#34;frequency&#34;) and self.frequency is not None and not force_rerun:
        return
    else:
        time.sleep(1)
        data_points, num_points = self.tooltip_scraper.scrape_dates_from_tooltips(num_points=7)
        logger.info(f&#34;Scraped {len(data_points)} data points from the chart, num_points target was {num_points}&#34;)
        
        if len(data_points) &gt; num_points:
            logger.debug(&#34;Successfully scraped start and end dates plus other data points to determine frequency of the time-series...&#34;)
            self.start_end = {
            &#39;start_date&#39;: data_points[-1][&#34;date&#34;],
            &#39;end_date&#39;: data_points[0][&#34;date&#34;],
            &#39;start_value&#39;: data_points[-1][&#34;value&#34;], 
            &#39;end_value&#39;: data_points[0][&#34;value&#34;]
            }
            dates = [point[&#34;date&#34;] for point in data_points][-2::-1]
            values = [point[&#34;value&#34;] for point in data_points][-2::-1]
            self.ripped_points = {&#34;dates&#34;: dates, &#34;values&#34;: values}

            diff = pd.Series(dates).diff().dropna().mode()[0]
            self.frequency = utils.map_frequency(diff)
            print(f&#34;Time delta between data points in the series appears to be approxiately: {diff.days} days, will use {self.frequency} frequency.&#34;)
            logger.debug(f&#34;Time series frequency appears to be: {diff.days}, {self.frequency}&#34;)

        else:
            logger.info(&#34;Error scraping data from tooltips..&#34;)
            logger.debug(&#34;Error scraping data from tooltips..&#34;)
            return None</code></pre>
</details>
<div class="desc"><p>Use the get_tooltip class to get the start and end dates and some other points of the time series using the tooltip box displayed on the chart.
Takes the latest num_points points from the chart and uses them to determine the frequency of the time series. The latest data is used
in case the earlier data is of lower frequency which can sometimes occurr.</p>
<p><strong>Parameters:</strong></p>
<ul>
<li>force_rerun (bool): Whether to force a rerun of the method to get the start and end dates and frequency of the time series again. The method
will not run again by default if done a second time and start_end and frequency attributes are already set. If the first run resulted in erroneous
assignation of these attributes, set this to True to rerun the method. However, something may need to be changed if it is not working&hellip;</li>
</ul></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.get_y_axis"><code class="name flex">
<span>def <span class="ident">get_y_axis</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_y_axis(self):
    &#34;&#34;&#34;Get y-axis values from chart to make a y-axis series with tick labels and positions (pixel positions).
    Also gets the limits of both axis in pixel co-ordinates. A series containing the y-axis values and their pixel positions (as index) is assigned
    to the &#34;y_axis&#34; attribute. The &#34;axis_limits&#34; attribute is made too &amp; is  dictionary containing the pixel co-ordinates of the max and min for both x and y axis.
    &#34;&#34;&#34;

    ##Get positions of y-axis gridlines
    y_heights = []
    self.full_chart = self.get_element(selector = &#34;#chart&#34;).get_attribute(&#34;outerHTML&#34;)
    self.chart_soup = BeautifulSoup(self.full_chart, &#39;html.parser&#39;)  #Make a bs4 object from the #chart element of the page.

    ## First get the pixel values of the max and min for both x and y axis.
    self.axis_limits = self.extract_axis_limits()

    ygrid = self.chart_soup.select(&#39;g.highcharts-grid.highcharts-yaxis-grid&#39;)
    gridlines = ygrid[1].findAll(&#39;path&#39;)
    for line in gridlines:
        y_heights.append(float(line.get(&#39;d&#39;).split(&#39; &#39;)[-1]))
    y_heights = sorted(y_heights)

    ##Get y-axis labels
    yax = self.chart_soup.select(&#39;g.highcharts-axis-labels.highcharts-yaxis-labels&#39;)
    textels = yax[1].find_all(&#39;text&#39;)

    # Replace metrc prefixes:
    yaxlabs = [utils.convert_metric_prefix(text.get_text()) if text.get_text().replace(&#39;,&#39;,&#39;&#39;).replace(&#39;.&#39;,&#39;&#39;).replace(&#39;-&#39;,&#39;&#39;).replace(&#39; &#39;,&#39;&#39;).isalnum() else text.get_text() for text in textels]
    logger.debug(f&#34;y-axis labels: {yaxlabs}&#34;)

    # convert to float...
    if any(isinstance(i, str) for i in yaxlabs):
        yaxlabs = [float(&#39;&#39;.join(filter(str.isdigit, i.replace(&#34;,&#34;, &#34;&#34;)))) if isinstance(i, str) else i for i in yaxlabs]
    pixheights = [float(height) for height in y_heights]
    pixheights.sort()

    ##Get px per unit for y-axis
    pxPerUnit = [abs((yaxlabs[i+1]- yaxlabs[i])/(pixheights[i+1]- pixheights[i])) for i in range(len(pixheights)-1)]
    average = sum(pxPerUnit)/len(pxPerUnit)
    self.unit_per_pix = average
    logger.debug(f&#34;Average px per unit for y-axis: {average}&#34;)  #Calculate the scaling for the chart so we can convert pixel co-ordinates to data values.

    yaxis = pd.Series(yaxlabs, index = pixheights, name = &#34;ytick_label&#34;)
    yaxis.index.rename(&#34;pixheight&#34;, inplace = True)
    try:
        yaxis = yaxis.astype(int)
    except:
        pass

    self.y_axis = yaxis
    if self.y_axis is not None:
        logger.debug(f&#34;Y-axis values scraped successfully.&#34;)
        logger.info(f&#34;Y-axis values scraped successfully.&#34;)
    return yaxis</code></pre>
</details>
<div class="desc"><p>Get y-axis values from chart to make a y-axis series with tick labels and positions (pixel positions).
Also gets the limits of both axis in pixel co-ordinates. A series containing the y-axis values and their pixel positions (as index) is assigned
to the "y_axis" attribute. The "axis_limits" attribute is made too &amp; is
dictionary containing the pixel co-ordinates of the max and min for both x and y axis.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.load_page"><code class="name flex">
<span>def <span class="ident">load_page</span></span>(<span>self, url, wait_time=5)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_page(self, url, wait_time=5):
    &#34;&#34;&#34;Load page and wait for it to be ready&#34;&#34;&#34;

    self.last_url = url
    self.series_name = url.split(&#34;/&#34;)[-1].replace(&#34;-&#34;, &#34; &#34;)
    try:
        self.driver.get(url)
        logger.debug(f&#34;Page loaded successfully: {url}&#34;)
        logger.info(f&#34;WebPage at {url} loaded successfully.&#34;)
        time.sleep(wait_time)  # Basic wait for page load
        self.full_page = self.get_page_source()
        self.page_soup = BeautifulSoup(self.full_page, &#39;html.parser&#39;)
        return True
    except Exception as e:
        print(f&#34;Error loading page: {str(e)}&#34;)
        logger.debug(f&#34;Error loading page: {str(e)}&#34;)
        return False</code></pre>
</details>
<div class="desc"><p>Load page and wait for it to be ready</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.make_x_index"><code class="name flex">
<span>def <span class="ident">make_x_index</span></span>(<span>self, force_rerun: bool = False, return_index: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_x_index(self, force_rerun: bool = False, return_index: bool = False):
    &#34;&#34;&#34;Make the DateTime Index for the series using the start and end dates scraped from the tooltips. 
    This does a few things and uses Selenium to scrape the dates from the tooltips on the chart as well as
    some more points to determine the frequency of the time series. It will take some time....
    &#34;&#34;&#34;
    
    print(&#34;Using selenium and toltip scraping to construct the date time index for the time-series, this&#39;ll take a bit...&#34;)
    self.get_xlims_from_tooltips(force_rerun = force_rerun)

    if self.start_end is not None:
        logger.info(f&#34;Start and end values scraped from tooltips: {self.start_end}&#34;)
        logger.debug(f&#34;Start and end values scraped from tooltips: {self.start_end}&#34;)
    else:
        print(&#34;Error: Start and end values not found...pulling out....&#34;)
        logger.debug(f&#34;Error: Start and end values not found...pulling out....&#34;)
        return None

    try:
        start_date = self.start_end[&#34;start_date&#34;]; end_date = self.start_end[&#34;end_date&#34;]
        dtIndex = self.dtIndex(start_date=start_date, end_date=end_date, ser_name=self.series_name)
        #print(&#34;Date index created successfully. Take a look at the final series: \n&#34;, dtIndex)
        logger.debug(f&#34;Date index created successfully: {dtIndex.index}&#34;)
        logger.info(f&#34;Date index created successfully.&#34;)
        if return_index:
            return dtIndex.index
    
    except Exception as e:
        print(f&#34;Error creating date index: {str(e)}&#34;)</code></pre>
</details>
<div class="desc"><p>Make the DateTime Index for the series using the start and end dates scraped from the tooltips.
This does a few things and uses Selenium to scrape the dates from the tooltips on the chart as well as
some more points to determine the frequency of the time series. It will take some time....</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.plot_series"><code class="name flex">
<span>def <span class="ident">plot_series</span></span>(<span>self,<br>annotation_text: str = None,<br>dpi: int = 300,<br>ann_box_pos: tuple = (0, -0.23))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_series(self, annotation_text: str = None, dpi: int = 300, ann_box_pos: tuple = (0, - 0.23)):
    &#34;&#34;&#34;
    Plots the time series data using pandas with plotly as the backend. Plotly is set as the pandas backend in __init__.py for tedata.
    If you want to use matplotlib or other plotting library don&#39;t use this method, plot the series attribute data directly. If using jupyter
    you can set 

    **Parameters**
    - annotation_text (str): Text to display in the annotation box at the bottom of the chart. Default is None. If None, the default annotation text
    will be created from the metadata.
    - dpi (int): The resolution of the plot in dots per inch. Default is 300.
    - ann_box_pos (tuple): The position of the annotation box on the chart. Default is (0, -0.23) which is bottom left.

    **Returns** None
    &#34;&#34;&#34;

    fig = self.series.plot()  # Plot the series using pandas, plotly needs to be set as the pandas plotting backend.

     # Existing title and label logic
    if hasattr(self, &#34;series_metadata&#34;):
        title = str(self.series_metadata[&#34;country&#34;]).capitalize() + &#34;: &#34; + str(self.series_metadata[&#34;title&#34;]).capitalize()
        ylabel = str(self.series_metadata[&#34;units&#34;]).capitalize()
        
        # Create default annotation text from metadata
        if annotation_text is None:
            annotation_text = (
                f&#34;Source: {self.series_metadata[&#39;source&#39;]}&lt;br&gt;&#34;
                f&#34;Original Source: {self.series_metadata[&#39;original_source&#39;]}&lt;br&gt;&#34;
                f&#34;Frequency: {self.series_metadata[&#39;frequency&#39;]}&#34;
            )
    else:
        title = &#34;Time Series Plot&#34;
        ylabel = &#34;Value&#34;
        annotation_text = annotation_text or &#34;Source: Trading Economics&#34;

    # Add text annotation to bottom left
    fig.add_annotation(
        text=annotation_text,
        xref=&#34;paper&#34;, yref=&#34;paper&#34;,
        x=ann_box_pos[0], y=ann_box_pos[1],
        showarrow=False, font=dict(size=10),
        align=&#34;left&#34;,  xanchor=&#34;left&#34;,
        yanchor=&#34;bottom&#34;, bgcolor=&#34;rgba(255, 255, 255, 0.8)&#34;,
        bordercolor=&#34;black&#34;, borderwidth=1)

    # Label x and y axis
    fig.update_layout(
        legend=dict(
        title_text=&#34;&#34;,  # Remove legend title
        orientation=&#34;h&#34;,
        yanchor=&#34;bottom&#34;,
        y=-0.2,  # Adjust this value to move the legend further down
        xanchor=&#34;center&#34;,
        x=0.5
        ),
        yaxis_title=ylabel,
        xaxis_title=&#34;&#34;,
        title = title)

    # Show the figure
    fig.show()
    self.plot = fig</code></pre>
</details>
<div class="desc"><p>Plots the time series data using pandas with plotly as the backend. Plotly is set as the pandas backend in <strong>init</strong>.py for tedata.
If you want to use matplotlib or other plotting library don't use this method, plot the series attribute data directly. If using jupyter
you can set </p>
<p><strong>Parameters</strong>
- annotation_text (str): Text to display in the annotation box at the bottom of the chart. Default is None. If None, the default annotation text
will be created from the metadata.
- dpi (int): The resolution of the plot in dots per inch. Default is 300.
- ann_box_pos (tuple): The position of the annotation box on the chart. Default is (0, -0.23) which is bottom left.</p>
<p><strong>Returns</strong> None</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.save_plot"><code class="name flex">
<span>def <span class="ident">save_plot</span></span>(<span>self,<br>filename: str = 'plot',<br>save_path: str = '/Users/jamesbishop/Documents/Python/Scraping/tedata',<br>dpi: int = 300,<br>format: str = 'png')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_plot(self, filename: str = &#34;plot&#34;, save_path: str = os.getcwd(), dpi: int = 300, format: str = &#34;png&#34;):
    &#34;&#34;&#34;Save the plot to a file. The plot must be created using the plot_series method. This method will save the plot as a PNG image file.

    **Parameters**
    - filename (str): The name of the file to save the plot to. Default is &#39;plot.png&#39;.
    - save_path (str): The directory to save the plot to. Default is the current working directory.
    - dpi (int): The resolution of the plot in dots per inch. Default is 300.
    - format (str): The format to save the plot in. Default is &#39;png&#39;. Other options are: &#34;html&#34;, &#34;bmp&#34;, &#34;jpeg&#34;, &#34;jpg&#34;.
    Use &#34;html&#34; to save as an interactive plotly plot.

    :Returns: None
    &#34;&#34;&#34;

    if hasattr(self, &#34;plot&#34;):
        if format == &#34;html&#34;:
            self.plot.write_html(f&#34;{save_path}{fdel}{filename}.html&#34;)
            logger.info(f&#34;Plot saved as {save_path}{fdel}{filename}.html&#34;)
        else:
            self.plot.write_image(f&#34;{save_path}{fdel}{filename}.{format}&#34;, format=format, scale=dpi/100, width = 1400, height = 500)
            logger.info(f&#34;Plot saved as {filename}&#34;)
    else:
        print(&#34;Error: Plot not found. Run plot_series() method to create a plot.&#34;)
        logger.debug(&#34;Error: Plot not found. Run plot_series() method to create a plot.&#34;)</code></pre>
</details>
<div class="desc"><p>Save the plot to a file. The plot must be created using the plot_series method. This method will save the plot as a PNG image file.</p>
<p><strong>Parameters</strong>
- filename (str): The name of the file to save the plot to. Default is 'plot.png'.
- save_path (str): The directory to save the plot to. Default is the current working directory.
- dpi (int): The resolution of the plot in dots per inch. Default is 300.
- format (str): The format to save the plot in. Default is 'png'. Other options are: "html", "bmp", "jpeg", "jpg".
Use "html" to save as an interactive plotly plot.</p>
<p>:Returns: None</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.scale_series"><code class="name flex">
<span>def <span class="ident">scale_series</span></span>(<span>self, right_way_up: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale_series(self, right_way_up: bool = True):
    &#34;&#34;&#34;Scale the series using the first and last values from the series pulled from the tooltip box. Uses the y axis limits and the max and min of the y axis
    to determine the scaling factor to convert pixel co-ordinates to data values. The scaling factor is stored in the self.axlims_upp attribute.&#34;&#34;&#34;

    if not right_way_up:
        max_val = self.y_axis.index.max()  # This should be the top pixel of the chart.
        self.series = utils.invert_series(self.series, max_val = max_val)

    if hasattr(self, &#34;start_end&#34;):
        y0 = self.start_end[&#34;start_value&#34;]; y1 = self.start_end[&#34;end_value&#34;]
        pix0 = self.series.iloc[0]; pix1 = self.series.iloc[-1]
        logger.debug(f&#34;scale_series method: Start value, end value: {y0}, {y1}, {pix0}, {pix1}, {pix0}, {pix1}&#34;)
        
        self.unit_per_px_alt = abs(y1 - y0) / abs(pix1 - pix0)  # Calculated from the start and end datapoints.
        
        if not hasattr(self, &#34;axis_limits&#34;):
            self.axis_limits = self.extract_axis_limits()
        ## Turns out that this formulation below is the best way to calculate the scaling factor for the chart.
        self.axlims_upp = (self.y_axis.iloc[-1] - self.y_axis.iloc[0]) / (self.axis_limits[&#34;y_max&#34;] - self.axis_limits[&#34;y_min&#34;])

        # if the start and end points are at similar values this will be problematic though. 
        logger.debug(f&#34;Start value, end value: {y0}, {y1}, pix0, pix1: {pix0}, {pix1}, &#34;
                     f&#34;data units per chart pixel from start &amp; end points: {self.unit_per_px_alt}, &#34;
                     f&#34;unit_per_pix calculated from the y axis ticks: {self.unit_per_pix}, &#34;
                     f&#34;inverse of that: {1/self.unit_per_pix}, &#34;
                     f&#34;unit_per_pix from axis limits and self.y_axis (probably best way): {self.axlims_upp}&#34;)

        self.unscaled_series = self.series.copy()
        ##Does the Y axis cross zero? Where is the zero point??
        x_intercept = utils.find_zero_crossing(self.series)

        if x_intercept:
            logger.debug(f&#34;Y axis Series does cross zero at:  {x_intercept}&#34;)
            pix0 = x_intercept

        for i in range(len(self.series)):
            self.series.iloc[i] = (self.series.iloc[i] - pix0)*self.axlims_upp + y0

        self.series = self.series
    else:
        print(&#34;start_end not found, run get_datamax_min() first.&#34;)
        logger.debug(&#34;start_end not found, run get_datamax_min() first.&#34;)
        return

    return self.series</code></pre>
</details>
<div class="desc"><p>Scale the series using the first and last values from the series pulled from the tooltip box. Uses the y axis limits and the max and min of the y axis
to determine the scaling factor to convert pixel co-ordinates to data values. The scaling factor is stored in the self.axlims_upp attribute.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.scrape_metadata"><code class="name flex">
<span>def <span class="ident">scrape_metadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_metadata(self):
    &#34;&#34;&#34;Scrape metadata from the page. This method scrapes metadata from the page and stores it in the &#39;metadata&#39; attribute. The metadata
    includes the title, indicator, country, length, frequency, source, , original source, id, start date, end date, min value, and max value of the series.
    It also scrapes a description of the series if available and stores it in the &#39;description&#39; attribute.
    &#34;&#34;&#34;

    self.metadata = {}
    logger.debug(f&#34;Scraping metadata for the series from the page...&#34;)

    try:
        self.metadata[&#34;units&#34;] = self.chart_soup.select_one(&#39;#singleIndChartUnit2&#39;).text
    except Exception as e:
        print(&#34;Units label not found: &#34;, {str(e)})
        self.metadata[&#34;units&#34;] = &#34;a.u&#34;
    
    try:
        self.metadata[&#34;original_source&#34;] = self.chart_soup.select_one(&#39;#singleIndChartUnit&#39;).text
    except Exception as e:
        print(&#34;original_source label not found: &#34;, {str(e)})
        self.metadata[&#34;original_source&#34;] = &#34;unknown&#34;

    if hasattr(self, &#34;series&#34;):
        if hasattr(self, &#34;page_soup&#34;):
            heads = self.page_soup.select(&#34;#ctl00_Head1&#34;)
            self.metadata[&#34;title&#34;] = heads[0].title.text.strip()
        else:
            self.metadata[&#34;title&#34;] = self.last_url.split(&#34;/&#34;)[-1].replace(&#34;-&#34;, &#34; &#34;)  # Use URL if can&#39;t find the title
        self.metadata[&#34;indicator&#34;] = self.last_url.split(&#34;/&#34;)[-1].replace(&#34;-&#34;, &#34; &#34;)  
        self.metadata[&#34;country&#34;] = self.last_url.split(&#34;/&#34;)[-2].replace(&#34;-&#34;, &#34; &#34;) 
        self.metadata[&#34;length&#34;] = len(self.series)
        self.metadata[&#34;frequency&#34;] = self.frequency  
        self.metadata[&#34;source&#34;] = &#34;Trading Economics&#34; 
        self.metadata[&#34;id&#34;] = &#34;/&#34;.join(self.last_url.split(&#34;/&#34;)[-2:])
        self.metadata[&#34;start_date&#34;] = self.series.index[0].strftime(&#34;%Y-%m-%d&#34;)
        self.metadata[&#34;end_date&#34;] = self.series.index[-1].strftime(&#34;%Y-%m-%d&#34;)
        self.metadata[&#34;min_value&#34;] = float(self.series.min())
        self.metadata[&#34;max_value&#34;] = float(self.series.max())
        print(&#34;Series metadata: &#34;, self.metadata)

    try:
        desc_card = self.page_soup.select_one(&#34;#item_definition&#34;)
        header_text = desc_card.select_one(&#39;.card-header&#39;).text.strip()
        if header_text.lower() == self.metadata[&#34;title&#34;].lower():
            self.metadata[&#34;description&#34;] = desc_card.select_one(&#39;.card-body&#39;).text.strip()
        else:
            print(&#34;Description card title does not match series title.&#34;)
            self.metadata[&#34;description&#34;] = &#34;Description not found.&#34;
    except Exception as e:
        print(&#34;Description card not found: &#34;, {str(e)})

    self.series_metadata = pd.Series(self.metadata)
    if self.metadata is not None:
        logger.debug(f&#34;Metadata scraped successfully: {self.metadata}&#34;)</code></pre>
</details>
<div class="desc"><p>Scrape metadata from the page. This method scrapes metadata from the page and stores it in the 'metadata' attribute. The metadata
includes the title, indicator, country, length, frequency, source, , original source, id, start date, end date, min value, and max value of the series.
It also scrapes a description of the series if available and stores it in the 'description' attribute.</p></div>
</dd>
<dt id="tedata.scraper.TE_Scraper.series_from_element"><code class="name flex">
<span>def <span class="ident">series_from_element</span></span>(<span>self,<br>element: str = None,<br>invert_the_series: bool = True,<br>return_series: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def series_from_element(self, element: str = None, invert_the_series: bool = True, return_series: bool = False):
    &#34;&#34;&#34;Extract series data from element text. This extracts the plotted series from the svg chart by taking the PATH 
    element of the data tarace on the chart. Series values are pixel co-ordinates on the chart.

    **Parameters:**

    - element (str): The element to extract data from. Will use self.current_element if not provided.
    - invert_the_series (bool): Whether to invert the series values.

    **Returns:**

    - series (pd.Series): The extracted series data.
    &#34;&#34;&#34;

    if element is None:
        element = self.current_element
    
    datastrlist = element.get_attribute(&#34;d&#34;).split(&#34; &#34;)
    ser = pd.Series(datastrlist)
    ser_num = pd.to_numeric(ser, errors=&#39;coerce&#39;).dropna()

    exvals = ser_num[::2]; yvals = ser_num[1::2]
    exvals = exvals.sort_values().to_list()
    yvals = yvals.to_list()
    series = pd.Series(yvals, index = exvals, name = &#34;Extracted Series&#34;)

    if invert_the_series:
        series = utils.invert_series(series, max_val = self.y_axis.index.max())
    self.series = series

    self.pix0 = self.series.iloc[0]; self.pix1 = self.series.iloc[-1]
    logger.debug(f&#34;Raw data series extracted successfully: {series.head()}&#34;)
    logger.info(f&#34;Raw data series extracted successfully.&#34;)
    if return_series:
        return series</code></pre>
</details>
<div class="desc"><p>Extract series data from element text. This extracts the plotted series from the svg chart by taking the PATH
element of the data tarace on the chart. Series values are pixel co-ordinates on the chart.</p>
<p><strong>Parameters:</strong></p>
<ul>
<li>element (str): The element to extract data from. Will use self.current_element if not provided.</li>
<li>invert_the_series (bool): Whether to invert the series values.</li>
</ul>
<p><strong>Returns:</strong></p>
<ul>
<li>series (pd.Series): The extracted series data.</li>
</ul></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tedata" href="index.html">tedata</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tedata.scraper.find_element_header_match" href="#tedata.scraper.find_element_header_match">find_element_header_match</a></code></li>
<li><code><a title="tedata.scraper.scrape_chart" href="#tedata.scraper.scrape_chart">scrape_chart</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tedata.scraper.TE_Scraper" href="#tedata.scraper.TE_Scraper">TE_Scraper</a></code></h4>
<ul class="">
<li><code><a title="tedata.scraper.TE_Scraper.BrowserType" href="#tedata.scraper.TE_Scraper.BrowserType">BrowserType</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.click_button" href="#tedata.scraper.TE_Scraper.click_button">click_button</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.close" href="#tedata.scraper.TE_Scraper.close">close</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.dtIndex" href="#tedata.scraper.TE_Scraper.dtIndex">dtIndex</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.extract_axis_limits" href="#tedata.scraper.TE_Scraper.extract_axis_limits">extract_axis_limits</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.find_max_button" href="#tedata.scraper.TE_Scraper.find_max_button">find_max_button</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.get_datamax_min" href="#tedata.scraper.TE_Scraper.get_datamax_min">get_datamax_min</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.get_element" href="#tedata.scraper.TE_Scraper.get_element">get_element</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.get_page_source" href="#tedata.scraper.TE_Scraper.get_page_source">get_page_source</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.get_xlims_from_tooltips" href="#tedata.scraper.TE_Scraper.get_xlims_from_tooltips">get_xlims_from_tooltips</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.get_y_axis" href="#tedata.scraper.TE_Scraper.get_y_axis">get_y_axis</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.load_page" href="#tedata.scraper.TE_Scraper.load_page">load_page</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.make_x_index" href="#tedata.scraper.TE_Scraper.make_x_index">make_x_index</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.plot_series" href="#tedata.scraper.TE_Scraper.plot_series">plot_series</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.save_plot" href="#tedata.scraper.TE_Scraper.save_plot">save_plot</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.scale_series" href="#tedata.scraper.TE_Scraper.scale_series">scale_series</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.scrape_metadata" href="#tedata.scraper.TE_Scraper.scrape_metadata">scrape_metadata</a></code></li>
<li><code><a title="tedata.scraper.TE_Scraper.series_from_element" href="#tedata.scraper.TE_Scraper.series_from_element">series_from_element</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
